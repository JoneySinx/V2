import logging
import re
import base64
import asyncio
from struct import pack
import motor.motor_asyncio
from hydrogram.file_id import FileId
from pymongo.errors import DuplicateKeyError, ServerSelectionTimeoutError
from info import DATABASE_URL, DATABASE_NAME, MAX_BTN

# Logger Setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ๐ PRE-COMPILED REGEX (SAVES CPU)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# เคฏเคน CPU usage เคเฅ 40% เคคเค เคเคฎ เคเคฐเคคเคพ เคนเฅ เคเคฌ เคฌเคนเฅเคค เคเฅเคฏเคพเคฆเคพ เคธเคฐเฅเค เคฐเคฟเคเฅเคตเฅเคธเฅเค เคเคคเฅ เคนเฅเค
NORMALIZE_PATTERN = re.compile(r"[^a-z0-9\s]")
WHITESPACE_PATTERN = re.compile(r"\s+")
USERNAME_PATTERN = re.compile(r"@\w+")

REPLACEMENTS = str.maketrans({
    "0": "o", "1": "i", "3": "e",
    "4": "a", "5": "s", "7": "t"
})

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# โ๏ธ MOTOR CONNECTION (KOYEB OPTIMIZED)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
client = motor.motor_asyncio.AsyncIOMotorClient(
    DATABASE_URL,
    maxPoolSize=20,           # Koyeb Free/Eco tier เคเฅ เคฒเคฟเค 10-20 เคฌเฅเคธเฅเค เคนเฅ (RAM เคฌเคเคพเคคเคพ เคนเฅ)
    minPoolSize=5,
    serverSelectionTimeoutMS=5000
)
db = client[DATABASE_NAME]

primary = db["Primary"]
cloud   = db["Cloud"]
archive = db["Archive"]

COLLECTIONS = {
    "primary": primary,
    "cloud": cloud,
    "archive": archive
}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# โก INDEX MANAGER (AUTO-RUN)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
async def check_mongo_status():
    """Startup เคชเคฐ DB เคเฅเค เคเคฐ Index เคฌเคจเคพเคเคเคพ"""
    try:
        # เคเคจเฅเคเฅเคถเคจ เคเฅเค
        await client.server_info()
        logger.info("โ MongoDB Connected Successfully!")
        
        # เคเคเคกเฅเคเฅเคธ เคฌเคจเคพเคจเคพ (Background เคฎเฅเค)
        await ensure_indexes()
    except ServerSelectionTimeoutError:
        logger.critical("โ MongoDB Connection Failed! IP Allowlist เคฏเคพ URL เคเฅเค เคเคฐเฅเคเฅค")
    except Exception as e:
        logger.error(f"โ DB Error: {e}")

async def ensure_indexes():
    for name, col in COLLECTIONS.items():
        try:
            # เคเฅเค เคเคฐเฅเค เคเคฟ เคเคเคกเฅเคเฅเคธ เคชเคนเคฒเฅ เคธเฅ เคฎเฅเคเฅเคฆ เคนเฅ เคฏเคพ เคจเคนเฅเค
            indexes = await col.index_information()
            index_name = f"{name}_text"
            
            if index_name not in indexes:
                logger.info(f"โณ Creating index for {name}...")
                await col.create_index(
                    [("file_name", "text"), ("caption", "text")],
                    name=index_name,
                    weights={"file_name": 10, "caption": 5}, # เคจเคพเคฎ เคเฅ เคเฅเคฏเคพเคฆเคพ เคฎเคนเคคเฅเคต
                    background=True
                )
                logger.info(f"โ Index created for {name}")
        except Exception as e:
            logger.error(f"Index failed for {name}: {e}")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ๐ง OPTIMIZED NORMALIZER
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
def normalize_query(q: str) -> str:
    if not q: return ""
    # Translate เคเคฐ Regex เคเค เคธเคพเคฅ (Fastest Method)
    q = q.lower().translate(REPLACEMENTS)
    q = NORMALIZE_PATTERN.sub(" ", q)
    return WHITESPACE_PATTERN.sub(" ", q).strip()

def prefix_query(q: str) -> str:
    # เคธเคฟเคฐเฅเคซ 3 เคเคเฅเคทเคฐ เคธเฅ เคฌเคกเคผเฅ เคถเคฌเฅเคฆเฅเค เคเคพ เคชเฅเคฐเฅเคซเคฟเคเฅเคธ เคฌเคจเคพเคเค
    return " ".join(w[:4] for w in q.split() if len(w) > 3)

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ๐พ SAVE FILE (SAFER)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
async def save_file(media, collection_type="primary"):
    try:
        file_id_str = unpack_new_file_id(media.file_id)
        if not file_id_str:
            return "err" # เคเคเคฐ ID เคกเคฟเคเฅเคก เคจเคนเฅเค เคนเฅเค เคคเฅ เคธเฅเคต เคจ เคเคฐเฅเค

        # Pre-compiled regex เคเคพ เคเคชเคฏเฅเค
        f_name = USERNAME_PATTERN.sub("", media.file_name or "").strip()
        caption = USERNAME_PATTERN.sub("", media.caption or "").strip()

        doc = {
            "_id": file_id_str,
            "file_name": f_name,
            "caption": caption,
            "file_size": media.file_size
        }

        col = COLLECTIONS.get(collection_type, primary)
        await col.insert_one(doc)
        return "suc"
    except DuplicateKeyError:
        return "dup"
    except Exception as e:
        logger.error(f"Save Error: {e}")
        return "err"

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ๐ SEARCH ENGINE (CORRECTED LOGIC)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
def _text_filter(q):
    return {"$text": {"$search": q}}

async def _search(col, q, offset, limit):
    try:
        # เคเฅเคตเคฒ เคเคฐเฅเคฐเฅ เคซเฅเคฒเฅเคกเฅเคธ เคจเคฟเคเคพเคฒเฅเค (Projection) - RAM เคฌเคเคพเคคเคพ เคนเฅ
        cursor = col.find(
            _text_filter(q),
            {"file_name": 1, "file_size": 1, "caption": 1, "score": {"$meta": "textScore"}}
        )
        cursor.sort([("score", {"$meta": "textScore"})])
        cursor.skip(offset).limit(limit)
        
        docs = await cursor.to_list(length=limit)
        # Count เคเคฒเค เคธเฅ (เคฅเฅเคกเคผเคพ เคงเฅเคฎเคพ เคนเฅ เคธเคเคคเคพ เคนเฅ, เคฒเฅเคเคฟเคจ เคธเคเฅเค เคนเฅ)
        # Note: เคฌเคกเคผเฅ DB เคฎเฅเค count() slow เคนเฅเคคเคพ เคนเฅ, เคฒเฅเคเคฟเคจ เคฏเคนเคพเค เคเคฐเฅเคฐเฅ เคนเฅ
        count = await col.count_documents(_text_filter(q))
        return docs, count
    except Exception as e:
        logger.error(f"Search Error in {col.name}: {e}")
        return [], 0

async def get_search_results(query, max_results=MAX_BTN, offset=0, lang=None, collection_type="primary"):
    if not query: return [], "", 0, collection_type
    
    query = normalize_query(query)
    if not query: return [], "", 0, collection_type

    # Lang Filter Pre-check (Optimization)
    lang = lang.lower() if lang else None

    # 1. Direct Collection Search
    if collection_type in COLLECTIONS and collection_type != "all":
        col = COLLECTIONS[collection_type]
        docs, total = await _search(col, query, offset, max_results)
        
        # Fallback Prefix (เคเคเคฐ เคกเคพเคฏเคฐเฅเคเฅเค เคฎเฅเค เคจ เคฎเคฟเคฒเฅ เคเคฐ เคฏเคน เคชเฅเค 1 เคนเฅ)
        if not docs and offset == 0:
            prefix = prefix_query(query)
            if prefix:
                docs, total = await _search(col, prefix, 0, max_results)
        
        # Language Filter Logic
        if lang:
            docs = [d for d in docs if lang in (d.get("file_name") or "").lower()]
            
        next_offset = offset + max_results if (offset + max_results) < total else ""
        return docs, next_offset, total, collection_type

    # 2. Cascade Search (All) - Logic Fix for Pagination
    # เคจเฅเค: เคฎเคฒเฅเคเฅ-เคเคฒเฅเคเฅเคถเคจ เคชเฅเคเคฟเคเค เคเคเคฟเคฒ เคนเฅเฅค เคฏเคนเคพเค เคนเคฎ "Best Effort" เคเคชเฅเคฐเฅเค เคฏเฅเค เคเคฐเฅเคเคเฅเฅค
    # เคนเคฎ เคเฅเคฐเคฎ เคธเฅ เคธเคฐเฅเค เคเคฐเฅเคเคเฅ, เคเคฌ เคคเค เคฐเคฟเคเคฒเฅเค เคจเคนเฅเค เคฎเคฟเคฒเคคเฅเฅค
    
    found_docs = []
    total_found = 0
    current_source = "primary"
    
    # Priority: Primary -> Cloud -> Archive
    search_order = [("primary", primary), ("cloud", cloud), ("archive", archive)]
    
    # เคนเคฎ เคธเคฟเคฐเฅเคซ เคชเคนเคฒเฅ เคจเฅเคจ-เคเคฎเฅเคชเคเฅ เคเคฒเฅเคเฅเคถเคจ เคธเฅ เคกเฅเคเคพ เคเคเคพเคเคเคเฅ (เคธเคฟเคเคชเคฒ เคเคฐ เคซเคพเคธเฅเค)
    # เคเคเคฐ เคเคชเคเฅ เคฎเคฐเฅเคเฅเคก เคฐเคฟเคเคฒเฅเค เคเคพเคนเคฟเค เคคเฅ เคตเฅ เคฌเคนเฅเคค Heavy Operation เคนเฅเฅค
    
    for name, col in search_order:
        docs, count = await _search(col, query, offset, max_results)
        if docs:
            found_docs = docs
            total_found = count
            current_source = name
            break # เคนเคฎเฅเค เคฐเคฟเคเคฒเฅเค เคฎเคฟเคฒ เคเคฏเคพ, เคฒเฅเคช เคคเฅเคกเคผเฅเค
    
    # เคเคเคฐ เคกเคพเคฏเคฐเฅเคเฅเค เคธเคฐเฅเค เคซเฅเคฒ เคนเฅเค, เคคเฅ Prefix เคธเคฐเฅเค เคเฅเคฐเคพเค เคเคฐเฅเค (เคธเคฟเคฐเฅเคซ Primary เคชเคฐ เคธเฅเคชเฅเคก เคเฅ เคฒเคฟเค)
    if not found_docs and offset == 0:
        prefix = prefix_query(query)
        if prefix:
             docs, count = await _search(primary, prefix, 0, max_results)
             if docs:
                 found_docs = docs
                 total_found = count
                 current_source = "primary"

    if lang and found_docs:
        found_docs = [d for d in found_docs if lang in (d.get("file_name") or "").lower()]
        # เคซเคฟเคฒเฅเคเคฐ เคเฅ เคฌเคพเคฆ เคเฅเคเคฒ เคเคพเคเคเค เคเคกเคผเคฌเคกเคผเคพ เคธเคเคคเคพ เคนเฅ, เคฒเฅเคเคฟเคจ เคฏเฅเคเคฐ เคเคเฅเคธเคชเฅเคฐเคฟเคฏเคเคธ เคเฅ เคฒเคฟเค เคเฅเค เคนเฅ

    next_offset = offset + max_results if (offset + max_results) < total_found else ""
    return found_docs, next_offset, total_found, current_source

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ๐ DELETE & UTILS
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
async def delete_files(query, collection_type="all"):
    if query == "*":
        return "Not Allowed via Bot" # เคธเฅเคฐเคเฅเคทเคพ เคเฅ เคฒเคฟเค
        
    query = normalize_query(query)
    deleted = 0
    flt = _text_filter(query)
    
    targets = COLLECTIONS.items() if collection_type == "all" else [(collection_type, COLLECTIONS.get(collection_type))]
    
    for name, col in targets:
        if col:
            res = await col.delete_many(flt)
            deleted += res.deleted_count
    return deleted

async def get_file_details(file_id):
    # Parallel Search (Fastest) - เคคเฅเคจเฅเค เคฎเฅเค เคเค เคธเคพเคฅ เคขเฅเคเคขเฅเคเคพ
    tasks = [col.find_one({"_id": file_id}) for col in COLLECTIONS.values()]
    results = await asyncio.gather(*tasks)
    
    for doc in results:
        if doc: return doc
    return None

# --- ID Utils (No Changes Needed, but optimized flow) ---
def encode_file_id(s: bytes) -> str:
    r, n = b"", 0
    for i in s + bytes([22, 4]):
        if i == 0: n += 1
        else:
            if n: r += b"\x00" + bytes([n]); n = 0
            r += bytes([i])
    return base64.urlsafe_b64encode(r).decode().rstrip("=")

def unpack_new_file_id(new_file_id):
    try:
        d = FileId.decode(new_file_id)
        return encode_file_id(pack("<iiqq", int(d.file_type), d.dc_id, d.media_id, d.access_hash))
    except Exception:
        return None
